<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Hidden in plain sight: VLMs overlook their visual representations - Research revealing VLM limitations on vision-centric tasks">
  <meta property="og:title" content="Hidden in plain sight: VLMs overlook their visual representations"/>
  <meta property="og:description" content="Research revealing that Vision Language Models perform substantially worse than their visual encoders on vision-centric tasks"/>
  <meta property="og:url" content="https://hidden-plain-sight.github.io/"/>
  <meta property="og:image" content="figures/main_results.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Hidden in plain sight: VLMs overlook their visual representations">
  <meta name="twitter:description" content="Research revealing that Vision Language Models perform substantially worse than their visual encoders on vision-centric tasks">
  <meta name="twitter:image" content="figures/main_results.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="vision language models, computer vision, machine learning, multimodal AI, VLM, COLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Hidden in plain sight: VLMs overlook their visual representations</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hidden in plain sight: VLMs overlook their visual representations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Authors</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Conference on Language Modeling (COLM) 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="paper.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
              </span>

              <span class="link-block">
                <a href="poster.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Poster</span>
              </a>
              </span>

              <span class="link-block">
                <a href="#" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code (Coming Soon)</span>
              </a>
              </span>

              <span class="link-block">
                <a href="#" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv (Coming Soon)</span>
              </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="figures/main_results.png" alt="Main results comparing VLM and visual encoder performance" class="center-image"/>
      <h2 class="subtitle has-text-centered">
        <strong>Vision Language Models (VLMs) perform substantially worse than their visual encoders on vision-centric tasks</strong>, often dropping to near-chance performance despite having access to powerful visual representations.
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Language provides a natural interface to specify and evaluate performance on visual tasks. 
            To realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information.
            Our work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. 
            Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. 
            We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. 
            We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the <em>entire</em> model, and they inherit their language biases. 
            Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Main Results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Findings</h2>
        
        <div class="content has-text-justified">
          <p>
            Our research reveals three critical observations about the performance gap between VLMs and their visual encoders:
          </p>
          
          <ol>
            <li>
              <strong>Universal Performance Drop:</strong> Shifting from standard visual probing strategies to a VLM-based evaluation results in a universal drop in performance, often to random chance. This trend persists across pretraining strategies for vision encoders, and does not preserve the rank-ordering of models on vision-centric benchmarks.
              <ul>
                <li>Vision representations throughout projector and LLM layers do not degrade and can still solve the task with a visual probing strategy.</li>
              </ul>
            </li>
            <li>
              <strong>Limited Prompt-tuning Benefits:</strong> Prompt-tuning the VLM improves performance marginally, but with diminishing improvements and does not close the gap with standard vision evaluation strategies.
            </li>
            <li>
              <strong>LLM as the Bottleneck:</strong> The LLM's ability to use its vision representations is a limiting factor in VLM performance. Across most tasks, finetuning the LLM results in higher accuracy than finetuning the projector or vision encoder.
              <ul>
                <li>VLM answer distributions largely reflect their blind answers. Finetuning the LLM (as opposed to the projector or ViT) also best overcomes these biases.</li>
              </ul>
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Language Bias Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">VLMs Inherit Language Model Biases</h2>
        
        <div class="content has-text-justified">
          <img src="figures/blind_mc_option2.png" alt="Analysis of language biases in VLMs showing answer distributions with and without visual input" class="center-image"/>
          <p>
            <strong>VLM choice behavior reflects the biases of their LLMs.</strong> 
            When comparing answer distributions with (blue) and without (orange) valid images, we find that behaviors largely reflect the pattern of choices in the blind baselines. 
            This evidence suggests that VLMs are not simply misusing their visual representations, but they inherit their blind biases.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Methodology -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluation Setup</h2>
        
        <div class="content has-text-justified">
          <p>
            We evaluate 4 different vision backbones: <strong>DINOv2 L/14</strong> (self-supervised), <strong>ViT-IN1k L/16</strong> (supervised), 
            and <strong>CLIP L/14</strong> and <strong>SigLIP L/14</strong> (both vision-language pretraining), paired with Vicu√±a v1.5. 
            We focus on 'vision-centric' tasks that test the model's ability to discern visual characteristics 
            without relying on language-level knowledge or domain expertise.
          </p>
          
          <h3 class="title is-4">Tasks Evaluated</h3>
          <div class="columns">
            <div class="column">
              <ul>
                <li><strong>Depth Estimation:</strong> Determining relative depth between objects in images</li>
                <li><strong>Semantic Correspondence:</strong> Matching object parts across different instances</li>
                <li><strong>Object Affordance:</strong> Understanding functional relationships between object parts</li>
              </ul>
            </div>
            <div class="column">
              <ul>
                <li><strong>Low-level Matching:</strong> Pattern matching across illumination and viewpoint changes</li>
                <li><strong>3D Object Awareness:</strong> Understanding 3D structure and relationships</li>
                <li><strong>Art Style Matching:</strong> Recognizing artistic styles across images</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Layer Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Visual Representations Remain Accessible</h2>
        
        <div class="content has-text-justified">
          <img src="figures/layer_analysis.png" alt="Layer-wise analysis showing visual representations remain effective throughout the VLM" class="center-image"/>
          <p>
            <strong>Analysis across different layers of the VLM architecture.</strong> 
            Visual representations remain accessible and effective throughout the entire model, 
            suggesting that the bottleneck lies in the language model's ability to utilize these representations 
            rather than in their degradation across layers.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Fine-tuning Analysis -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Fine-tuning Analysis</h2>
        
        <div class="content has-text-justified">
          <img src="figures/ft.png" alt="Fine-tuning analysis showing LLM fine-tuning is most effective" class="center-image"/>
          <p>
            <strong>The LLM component is the primary bottleneck.</strong> 
            When fine-tuning different components of the VLM (vision encoder, projector, or LLM), 
            we find that fine-tuning the LLM provides the most substantial improvements in performance, 
            confirming that the bottleneck lies in the language model's ability to interpret visual representations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Failure Cases -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Understanding Failure Modes</h2>
        
        <div class="content has-text-justified">
          <img src="figures/failures.png" alt="Example failure cases for VLMs and vision encoders" class="center-image"/>
          <p>
            <strong>Example failure cases for VLMs (left), vision encoders (center), and both (right).</strong> 
            We observe common failure modes including confusion from similar local structure and challenges with 
            small objects that may be too small to be individually encoded in high fidelity.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Conference Poster -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Conference Poster</h2>
        <div class="content has-text-centered">
          <p><a href="poster.pdf" target="_blank" class="button is-large is-primary">
            <span class="icon"><i class="fas fa-external-link-alt"></i></span>
            <span>View Full Poster</span>
          </a></p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hiddenplainsight2025,
  title={Hidden in plain sight: VLMs overlook their visual representations},
  author={Anonymous Authors},
  journal={Conference on Language Modeling (COLM)},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> 
        which was adopted from the <a href="https://nerfies.github.io/" target="_blank">Nerfies</a> project page.
        You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br>
        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
